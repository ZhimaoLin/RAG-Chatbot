{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Papers to Pinecone Vector Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "\n",
    "If you are using MacOS, please use `pip3`.\n",
    "\n",
    "`-qU` means `quiet` and `Upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.276 \\\n",
    "    openai==0.27.10 \\\n",
    "    tiktoken==0.4.0 \\\n",
    "    pinecone-client==2.2.2 \\\n",
    "    wikipedia==1.4.0 \\\n",
    "    pypdf==3.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings   \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "import pinecone\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from config import OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME, EMBEDDING_MODEL, SPLITTER_CHUNK_SIZE, SPLITTER_CHUNK_OVERLAP, UPLOAD_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable\n",
    "\n",
    "- `PAPER_LIST`: Store file paths of upload papers into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_LIST = [\"data/paper1.pdf\", \"data/paper2.pdf\", \"data/paper3.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_match(result):\n",
    "    for match in result['matches']:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Score: {match['score']:.2f} \\t Source: {match['metadata']['source']} \\t Page: {int(match['metadata']['page'])}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{match['metadata']['text']}\")\n",
    "        print(\"=\"*60)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize OpenAI Embedding\n",
    "\n",
    "Here `text-embedding-ada-002` embedding is used by default. Please refer to [OpenAI embedding document](https://platform.openai.com/docs/guides/embeddings/embedding-models) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "OpenAI initialization: OK\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"OpenAI initialization: OK\")\n",
    "print(\"=\"*30)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pinecone\n",
    "\n",
    "If the index does not exist in your Pinecone, it will automatically create a new one. \n",
    "\n",
    "- `metric='cosine'`: This is often used to find similarities between different documents. The advantage is that the scores are normalized to [-1,1] range. You can choose other options listed [here](https://docs.pinecone.io/docs/indexes#distance-metrics).\n",
    "- `dimension=1536`: The OpenAI `text-embedding-ada-002` embedding has a dimension of 1536\n",
    "- There is a limitation for the free plan for Pinecone. Please refer to the [starter plan](https://docs.pinecone.io/docs/indexes#starter-plan) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Pinecone initialization: OK\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")\n",
    "\n",
    "if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
    "    # we create a new index if it doesn't exist\n",
    "    pinecone.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        metric='cosine',\n",
    "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    time.sleep(1)\n",
    "\n",
    "pinecone_index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "pinecone_stats = pinecone_index.describe_index_stats()\n",
    "print(\"=\"*30)\n",
    "print(\"Pinecone initialization: OK\")\n",
    "print(pinecone_stats)\n",
    "print(\"=\"*30)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload PDF Files\n",
    "\n",
    "### Load PDFs\n",
    "\n",
    "`PyPDF` is used to load the PDFs and [Tiktoken Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/split_by_token#tiktoken) is used to split the document by tokens. This tokenizer is created by OpenAI, so it is more accurate for OpenAI models. \n",
    "\n",
    "- `chunk_size=60`: Combine 60 tokens into a chunk in order to make each chunk has a reasonable amount of context. Tested with `40`, `50`, `60`, and `100`, and `60` offers a reasonable context size and a search resolution.\n",
    "- `chunk_overlap=15`: Considering most papers have two columns, so there will be more cut-off and broken texts. 1/4 of the chunk size is used for overlapping to mitigate the effect of cut-off and broken texts.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "Then, each chunk is embedded to a vector with a dimension of 1536 using the OpenAI embedding model mentioned above. However, in order to upload the vectors to Pinecone, the vectors has to be in [this format](https://docs.pinecone.io/docs/python-client#indexupsert). \n",
    "\n",
    "```\n",
    "upsert_response = index.upsert(\n",
    "   vectors=[\n",
    "       {'id': \"vec1\", \"values\":[0.1, 0.2, 0.3, 0.4], \"metadata\": {'genre': 'drama'}},\n",
    "       {'id': \"vec2\", \"values\":[0.2, 0.3, 0.4, 0.5], \"metadata\": {'genre': 'action'}},\n",
    "   ],\n",
    "   namespace='example-namespace'\n",
    ")\n",
    "```\n",
    "\n",
    "- Here, `str(uuid.uuid4())` is used as `id` instead of a string of an incrementing integer since the number of vectors is huge. Approximately, 500 vectors per PDF. \n",
    "- The original text, file name, and the page number of the text are stored as metadata.\n",
    "  - For demo purpose, only this information is stored. More information such as paper title, publish date, and authors etc. can be stored as metadata.\n",
    "\n",
    "### Batch Upload\n",
    "\n",
    "Due to efficiency, vectors are uploaded to Pinecone in batches. By default, the batch size is `32`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [data/paper1.pdf]\n",
      "Pages shape: 42\n",
      "Total number: 736\n",
      "Uploaded batch [0 : 32]\n",
      "Uploaded batch [32 : 64]\n",
      "Uploaded batch [64 : 96]\n",
      "Uploaded batch [96 : 128]\n",
      "Uploaded batch [128 : 160]\n",
      "Uploaded batch [160 : 192]\n",
      "Uploaded batch [192 : 224]\n",
      "Uploaded batch [224 : 256]\n",
      "Uploaded batch [256 : 288]\n",
      "Uploaded batch [288 : 320]\n",
      "Uploaded batch [320 : 352]\n",
      "Uploaded batch [352 : 384]\n",
      "Uploaded batch [384 : 416]\n",
      "Uploaded batch [416 : 448]\n",
      "Uploaded batch [448 : 480]\n",
      "Uploaded batch [480 : 512]\n",
      "Uploaded batch [512 : 544]\n",
      "Uploaded batch [544 : 576]\n",
      "Uploaded batch [576 : 608]\n",
      "Uploaded batch [608 : 640]\n",
      "Uploaded batch [640 : 672]\n",
      "Uploaded batch [672 : 704]\n",
      "Uploaded batch [704 : 736]\n",
      "Processing [data/paper2.pdf]\n",
      "Pages shape: 26\n",
      "Total number: 523\n",
      "Uploaded batch [0 : 32]\n",
      "Uploaded batch [32 : 64]\n",
      "Uploaded batch [64 : 96]\n",
      "Uploaded batch [96 : 128]\n",
      "Uploaded batch [128 : 160]\n",
      "Uploaded batch [160 : 192]\n",
      "Uploaded batch [192 : 224]\n",
      "Uploaded batch [224 : 256]\n",
      "Uploaded batch [256 : 288]\n",
      "Uploaded batch [288 : 320]\n",
      "Uploaded batch [320 : 352]\n",
      "Uploaded batch [352 : 384]\n",
      "Uploaded batch [384 : 416]\n",
      "Uploaded batch [416 : 448]\n",
      "Uploaded batch [448 : 480]\n",
      "Uploaded batch [480 : 512]\n",
      "Uploaded batch [512 : 523]\n",
      "Processing [data/paper3.pdf]\n",
      "Pages shape: 20\n",
      "Total number: 344\n",
      "Uploaded batch [0 : 32]\n",
      "Uploaded batch [32 : 64]\n",
      "Uploaded batch [64 : 96]\n",
      "Uploaded batch [96 : 128]\n",
      "Uploaded batch [128 : 160]\n",
      "Uploaded batch [160 : 192]\n",
      "Uploaded batch [192 : 224]\n",
      "Uploaded batch [224 : 256]\n",
      "Uploaded batch [256 : 288]\n",
      "Uploaded batch [288 : 320]\n",
      "Uploaded batch [320 : 344]\n"
     ]
    }
   ],
   "source": [
    "for file_path in PAPER_LIST:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    print(f\"Processing [{file_path}]\")\n",
    "    print(f\"Pages shape: {len(pages)}\")\n",
    "\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=SPLITTER_CHUNK_SIZE, \n",
    "        chunk_overlap=SPLITTER_CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    source = pages[0].metadata[\"source\"]\n",
    "\n",
    "    total_sentences = []\n",
    "    page_number_list = []\n",
    "    for idx, page in enumerate(pages):\n",
    "        page_num = page.metadata[\"page\"] + 1\n",
    "        sentences = text_splitter.split_text(page.page_content)\n",
    "        total_sentences += sentences\n",
    "        page_number_list += [page_num] * len(sentences)\n",
    "\n",
    "    # Due to OpenAPI rate limitation, I have to embed multiple chunks at the same time\n",
    "    paper_embedding = embedding_model.embed_documents(total_sentences)\n",
    "\n",
    "    # Reformat the vectors\n",
    "    to_upsert = []\n",
    "    for i, sentence_vector in enumerate(paper_embedding):\n",
    "        to_upsert.append({\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"values\": sentence_vector,\n",
    "            \"metadata\": {\n",
    "                            \"text\": total_sentences[i],\n",
    "                            \"source\": source,\n",
    "                            \"page\": page_number_list[i]\n",
    "                        }\n",
    "        })\n",
    "\n",
    "    # Upload the vectors in baches\n",
    "    batch_size = UPLOAD_BATCH_SIZE\n",
    "    n = len(to_upsert)\n",
    "    print(f\"Total number: {n}\")\n",
    "\n",
    "    for i in range(0, n, batch_size):\n",
    "        if i + batch_size <= n:\n",
    "            batch = to_upsert[i: i+batch_size]     \n",
    "        else:\n",
    "            batch = to_upsert[i:]\n",
    "\n",
    "        pinecone_index.upsert(vectors=batch)\n",
    "        print(f\"Uploaded batch [{i} : {min(n, i+batch_size)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Automatically test the upload results using some queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Test 1: How to treat patient with ACHD?\n",
      "==============================\n",
      "============================================================\n",
      "Score: 0.86 \t Source: data/paper1.pdf \t Page: 6\n",
      "============================================================\n",
      "\n",
      "effective and/or bene ﬁcial.\n",
      "Practical tip. ACHD patients should be referred early\n",
      "and followed by transplant and ACHD teams to determine\n",
      "optimal timing for transplant listing. HTx should be\n",
      "considered as a potential management strategy in ACHD\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Score: 0.83 \t Source: data/paper1.pdf \t Page: 7\n",
      "============================================================\n",
      " centres with multidisciplinary\n",
      "expertise in congenital heart disease and trans-plantation (Strong Recommendation, Low-Quality\n",
      "Evidence).\n",
      "Values and preferences. Factors unique to patients\n",
      "with ACHD must be considered during HTx assessment.\n",
      "RECOMMENDATION\n",
      "17\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Score: 0.83 \t Source: data/paper1.pdf \t Page: 7\n",
      "============================================================\n",
      " HLivT has similar outcomes compared with non-ACHD patients.78It should be emphasized that observationalRECOMMENDATION\n",
      "16. We recommend patients with ACHD undergo eval-\n",
      "uation and HTx at centres with multidisciplinary\n",
      "expertise in congenital heart disease and\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How to treat patient with ACHD?\"\n",
    "print(\"=\"*30)\n",
    "print(f\"Test 1: {query}\")\n",
    "print(\"=\"*30)\n",
    "query_embedding = embedding_model.embed_documents([query])\n",
    "res = pinecone_index.query(query_embedding, top_k=3, include_metadata=True)\n",
    "print_match(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Test 2: How to reduce the cardiorenal risk?\n",
      "==============================\n",
      "============================================================\n",
      "Score: 0.88 \t Source: data/paper2.pdf \t Page: 9\n",
      "============================================================\n",
      "iorenal Risk Reduction\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Score: 0.88 \t Source: data/paper2.pdf \t Page: 7\n",
      "============================================================\n",
      ". 1159\n",
      "CCS Guideline for Cardiorenal Risk Reduction\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Score: 0.88 \t Source: data/paper2.pdf \t Page: 15\n",
      "============================================================\n",
      " 1167\n",
      "CCS Guideline for Cardiorenal Risk Reduction\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How to reduce the cardiorenal risk?\"\n",
    "print(\"=\"*30)\n",
    "print(f\"Test 2: {query}\")\n",
    "print(\"=\"*30)\n",
    "query_embedding = embedding_model.embed_documents([query])\n",
    "res = pinecone_index.query(query_embedding, top_k=3, include_metadata=True)\n",
    "print_match(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Test 3: How to diagnose Resistant Hypertension?\n",
      "==============================\n",
      "============================================================\n",
      "Score: 0.91 \t Source: data/paper3.pdf \t Page: 4\n",
      "============================================================\n",
      "R\n",
      "C\n",
      "Conﬁrm diagnosis of true resistant hypertension\n",
      "Figure 1. Diagnostic algorithm for a patient with suspected resistant hypertension. ABPM, ambulatory blood pressure monitoring; BP, blood\n",
      "pressure; HT, hypertension. *Three or more drugs, at optimally tolerated dosages\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Score: 0.90 \t Source: data/paper3.pdf \t Page: 4\n",
      "============================================================\n",
      "resistant\" hypertension\n",
      "Refer to HT specialistConsider referral to HT specialist\n",
      "R\n",
      "C\n",
      "Conﬁrm diagnosis of true resistant hypertension\n",
      "Figure 1. Diagnostic algorithm for a patient with suspected resistant hypertension. ABPM, ambulatory blood pressure monitoring; BP, blood\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Score: 0.88 \t Source: data/paper3.pdf \t Page: 3\n",
      "============================================================\n",
      " for uncontrolled vs controlled RHT.20\n",
      "Diagnosis of Resistant Hypertension\n",
      "The diagnosis of RHT should take into account proper\n",
      "ofﬁce and out-of-of ﬁce BP measurement, optimization of\n",
      "pharmacotherapy taking into consideration clinical inertia,\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How to diagnose Resistant Hypertension?\"\n",
    "print(\"=\"*30)\n",
    "print(f\"Test 3: {query}\")\n",
    "print(\"=\"*30)\n",
    "query_embedding = embedding_model.embed_documents([query])\n",
    "res = pinecone_index.query(query_embedding, top_k=3, include_metadata=True)\n",
    "print_match(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
