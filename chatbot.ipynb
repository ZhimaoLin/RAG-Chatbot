{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   langchain==0.0.276 \\\n",
    "#   openai==0.27.10 \\\n",
    "#   tiktoken==0.4.0 \\\n",
    "#   sentence-transformers==2.2.2 \\\n",
    "#   spacy==3.6.1 \\\n",
    "#   nltk==3.8.1 \\\n",
    "#   pinecone-client==2.2.2 \\\n",
    "#   pypdf==3.15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings   \n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter, TokenTextSplitter, SpacyTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory\n",
    "\n",
    "\n",
    "import pinecone\n",
    "import time\n",
    "# import itertools\n",
    "# import uuid\n",
    "# from tqdm.autonotebook import tqdm\n",
    "\n",
    "from config import OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME, EMBEDDING_MODEL, SPLITTER_CHUNK_SIZE, SPLITTER_CHUNK_OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    print(\"=\"*30)\n",
    "    print(\" \"*10 + \"Question\")\n",
    "    print(\"=\"*30)\n",
    "    print(result[\"question\"])\n",
    "    print(\"=\"*30)\n",
    "    print()\n",
    "\n",
    "    print(\"=\"*30)\n",
    "    print(\" \"*10 + \"Answer\")\n",
    "    print(\"=\"*30)\n",
    "    print(result[\"answer\"])\n",
    "    print(\"=\"*30)\n",
    "    print()\n",
    "\n",
    "    sources = result[\"source_documents\"]\n",
    "\n",
    "    for i in range(min(3, len(sources))):\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Source [{i+1}] \\t File: [{sources[i].metadata['source']}] \\t Page: [{int(sources[i].metadata['page'])}]\")\n",
    "        print(\"=\"*30)\n",
    "        print(sources[i].page_content)\n",
    "        print(\"=\"*30)\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open AI Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinecone Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04929,\n",
       " 'namespaces': {'': {'vector_count': 4929}},\n",
       " 'total_vector_count': 4929}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")\n",
    "\n",
    "if PINECONE_INDEX_NAME not in pinecone.list_indexes():\n",
    "    # we create a new index if it doesn't exist\n",
    "    pinecone.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        metric='cosine',\n",
    "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    time.sleep(1)\n",
    "\n",
    "pinecone_index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[api doc](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.pinecone.Pinecone.html#langchain.vectorstores.pinecone.Pinecone.as_retriever)\n",
    "\n",
    "[What is MMR](https://medium.com/tech-that-works/maximal-marginal-relevance-to-rerank-results-in-unsupervised-keyphrase-extraction-22d95015c7c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(pinecone_index, embedding_model, \"text\")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "                    \"k\": 5,\n",
    "                    \"lambda_mult\": 0.5, # the optimal mix of diversity and accuracy in the result set\n",
    "                    }\n",
    ")\n",
    "# retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"score_threshold\": 0.5})\n",
    "\n",
    "\n",
    "# memory = ConversationSummaryMemory(llm=chat_model,memory_key=\"chat_history\",return_messages=True)\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", input_key='question', output_key='answer', return_messages=True)\n",
    "# memory = ConversationBufferWindowMemory(k=12, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "memory.clear()\n",
    "conversation_qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "          Question\n",
      "==============================\n",
      "Why bolt looseness is harmful?\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "          Answer\n",
      "==============================\n",
      "Bolt looseness can be harmful because it can lead to failures in bolted joints, especially in critical structures like hydropower plants. When bolts become loose, the integrity and stability of the joint are compromised, which can result in catastrophic disasters such as flooding and power shortages. Loose bolts can also affect the performance and efficiency of machinery and equipment. Therefore, it is important to monitor and detect bolt looseness to prevent potential hazards and ensure the safety and reliability of structures and systems.\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "Source [1] \t File: [data/mypaper.pdf] \t Page: [6]\n",
      "==============================\n",
      ". The most direct method of monitoring bolt looseness is to\n",
      "measure the axial force of the bolt using strain gauges; however, the complication of installation and\n",
      "wiring introduces huge risks\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "Source [2] \t File: [data/mypaper.pdf] \t Page: [2]\n",
      "==============================\n",
      " are widely used in hydropower plants. A failure of the bolted joint, especially the\n",
      "ones on the hydro turbine head cover, can cause catastrophic disasters such as flooding and power\n",
      "shortage.\n",
      "==============================\n",
      "\n",
      "==============================\n",
      "Source [3] \t File: [data/mypaper.pdf] \t Page: [9]\n",
      "==============================\n",
      " is shown in Figure 3.1. When the bolt is tight, the depth of the hole is\n",
      "deeper due to the reaction force of the clamped material. When the bolt becomes loose,\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Why bolt looseness is harmful?\"\n",
    "res = conversation_qa_chain({\"question\": query})\n",
    "\n",
    "print_result(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\npage_content\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\Test\\RAG-Chatbot\\chatbot.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWho is Bill Gates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m res \u001b[39m=\u001b[39m vectorstore\u001b[39m.\u001b[39;49mmax_marginal_relevance_search(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     query\u001b[39m=\u001b[39;49mquery,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     k\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     fetch_k\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     lambda_mult\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Test/RAG-Chatbot/chatbot.ipynb#Y141sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(res)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\vectorstores\\pinecone.py:301\u001b[0m, in \u001b[0;36mPinecone.max_marginal_relevance_search\u001b[1;34m(self, query, k, fetch_k, lambda_mult, filter, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39m\"\"\"Return docs selected using the maximal marginal relevance.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[39mMaximal marginal relevance optimizes for similarity to query AND diversity\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39m    List of Documents selected by maximal marginal relevance.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embed_query(query)\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_marginal_relevance_search_by_vector(\n\u001b[0;32m    302\u001b[0m     embedding, k, fetch_k, lambda_mult, \u001b[39mfilter\u001b[39;49m, namespace\n\u001b[0;32m    303\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\vectorstores\\pinecone.py:269\u001b[0m, in \u001b[0;36mPinecone.max_marginal_relevance_search_by_vector\u001b[1;34m(self, embedding, k, fetch_k, lambda_mult, filter, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m mmr_selected \u001b[39m=\u001b[39m maximal_marginal_relevance(\n\u001b[0;32m    263\u001b[0m     np\u001b[39m.\u001b[39marray([embedding], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    264\u001b[0m     [item[\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m]],\n\u001b[0;32m    265\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[0;32m    266\u001b[0m     lambda_mult\u001b[39m=\u001b[39mlambda_mult,\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m selected \u001b[39m=\u001b[39m [results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m mmr_selected]\n\u001b[1;32m--> 269\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    270\u001b[0m     Document(page_content\u001b[39m=\u001b[39mmetadata\u001b[39m.\u001b[39mpop((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_key)), metadata\u001b[39m=\u001b[39mmetadata)\n\u001b[0;32m    271\u001b[0m     \u001b[39mfor\u001b[39;00m metadata \u001b[39min\u001b[39;00m selected\n\u001b[0;32m    272\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\vectorstores\\pinecone.py:270\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m mmr_selected \u001b[39m=\u001b[39m maximal_marginal_relevance(\n\u001b[0;32m    263\u001b[0m     np\u001b[39m.\u001b[39marray([embedding], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    264\u001b[0m     [item[\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m]],\n\u001b[0;32m    265\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[0;32m    266\u001b[0m     lambda_mult\u001b[39m=\u001b[39mlambda_mult,\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m selected \u001b[39m=\u001b[39m [results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m mmr_selected]\n\u001b[0;32m    269\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 270\u001b[0m     Document(page_content\u001b[39m=\u001b[39;49mmetadata\u001b[39m.\u001b[39;49mpop((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_text_key)), metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[0;32m    271\u001b[0m     \u001b[39mfor\u001b[39;00m metadata \u001b[39min\u001b[39;00m selected\n\u001b[0;32m    272\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\load\\serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Document\npage_content\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "query = \"Who is Bill Gates?\"\n",
    "res = vectorstore.max_marginal_relevance_search(\n",
    "    query=query,\n",
    "    k=4,\n",
    "    fetch_k=20,\n",
    "    lambda_mult=0.5\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
